{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entries_for_each_record(input_file, output_file):\n",
    "    with open (input_file) as fd:\n",
    "        text = fd.read()\n",
    "        section_index = {}\n",
    "\n",
    "        for each in all_section:\n",
    "            temp = text.find(each)\n",
    "            if temp != -1:\n",
    "                section_index[temp] = each\n",
    "        print(section_index)\n",
    "        sorted(section_index.items())\n",
    "    boundaries = [key for key in section_index.keys()]\n",
    "\n",
    "    fd = open(output_file,'r')\n",
    "    text = fd.readlines()\n",
    "    fd.close() \n",
    "\n",
    "    t_note_id = []\n",
    "    t_section_concept_id = []\n",
    "    t_offsets = []\n",
    "    t_lexical_variant = []\n",
    "    t_note_nlp_concept_id = []\n",
    "    t_nlp_system = []\n",
    "    t_nlp_date_time = []\n",
    "    t_term_exists = []\n",
    "\n",
    "    number = re.search('[0-9]{1,}',output_file)\n",
    "    number = number[0]\n",
    "    for each in text:\n",
    "        sentence = each.split('|')\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i]=='TX':\n",
    "                offset_in_sentence = i+1\n",
    "                concept_pos = i-1\n",
    "                cui_pos = 4\n",
    "        temp = sentence[offset_in_sentence].replace('[','')\n",
    "        temp = temp.replace(']','')\n",
    "        temp = temp.replace(',',';')\n",
    "        offsets = temp.split(';')\n",
    "        offsets = [aa.split('/')[0] for aa in offsets]\n",
    "        length = len(offsets)\n",
    "        #now we have many offsets, consider lexical_variant\n",
    "        temp = sentence[concept_pos].split('-')\n",
    "        concept = temp[0]\n",
    "        del temp\n",
    "        concept = concept.strip('[').strip('\"')\n",
    "        lexical_variant = [concept for a in range(length)]\n",
    "        # for cui\n",
    "        cui = sentence[cui_pos]\n",
    "        note_nlp_concept_id = [cui for a in range(length)]\n",
    "        # for note_id\n",
    "        note_id = [number for a in range(length)]\n",
    "        # for nlp_system\n",
    "        nlp_system = ['Metamap' for a in range(length)]\n",
    "        # for date\n",
    "        date_time = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        nlp_date_time = [date_time for a in range(length)]\n",
    "        #for term exists\n",
    "        term_exists = ['Y' for a in range(length)]\n",
    "        #for section concept\n",
    "        section_concept_id = []\n",
    "        for i in range(length):\n",
    "            j = 0\n",
    "            while j < len(boundaries):\n",
    "                if int(offsets[i]) < boundaries[j]:\n",
    "                    temp_id = j\n",
    "                    break\n",
    "                j+=1\n",
    "                if j==len(boundaries):\n",
    "                    temp_id = j\n",
    "                    break\n",
    "            if temp_id == 0:\n",
    "                section_concept_id.append('0')\n",
    "            else:\n",
    "                temp_id = boundaries[temp_id-1]\n",
    "                temp_section = section_index[temp_id]\n",
    "                section_id = all_sections.index(temp_section)+1\n",
    "                section_concept_id.append(section_id)\n",
    "        # after this we have:\n",
    "        # note_id, section_concept, offset, lexical, note_nlp_concept_id, nlp_system, nlp_date, term_exists\n",
    "        t_note_id.extend(note_id)\n",
    "        t_section_concept_id.extend(section_concept_id)\n",
    "        t_offsets.extend(offsets)\n",
    "        t_lexical_variant.extend(lexical_variant)\n",
    "        t_note_nlp_concept_id.extend(note_nlp_concept_id)\n",
    "        t_nlp_system.extend(nlp_system)\n",
    "        t_nlp_date_time.extend(nlp_date_time)\n",
    "        t_term_exists.extend(term_exists)\n",
    "\n",
    "    t_snippet =  [0 for a in range(len(t_term_exists))]\n",
    "    t_note_nlp_id = list(range(1,len(t_term_exists)+1))\n",
    "\n",
    "    # write to csv file\n",
    "    data_row = []\n",
    "    for i in range(len(t_snippet)):\n",
    "        temp = [t_note_nlp_id[i], t_note_id[i], t_section_concept_id[i], t_snippet[i], \n",
    "                    t_offsets[i], t_lexical_variant[i], t_nlp_date_time[i], t_term_exists[i]]\n",
    "        data_row.append(temp)\n",
    "    for each in data_row:\n",
    "        writer.writerow(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "haha\n",
      "{155: 'Discharge Date', 354: 'SECONDARY DIAGNOSES', 528: 'HISTORY OF PRESENT ILLNESS', 3194: 'PAST MEDICAL HISTORY', 4009: 'FAMILY HISTORY', 3879: 'SOCIAL HISTORY', 3520: 'ALLERGIES'}\n"
     ]
    }
   ],
   "source": [
    "columns = ['note_nlp_id','note_id','section_concept_id', 'snippet', 'offset', \n",
    "           'lexical_variant', 'note_nlp_concept_id','nlp_system','nlp_date_time', 'term_exists']\n",
    "all_sections = ['Discharge Date',  'PRINCIPAL DIAGNOSIS',     'SECONDARY DIAGNOSES', 'HISTORY OF PRESENT ILLNESS',\n",
    "                'PRE-ADMISSION MEDICATIONS', 'PAST MEDICAL HISTORY', 'FAMILY HISTORY',   'SOCIAL HISTORY',\n",
    "                'ALLERGIES', 'ADMISSION PHYSICAL EXAMINATION',  'STUDIES:' ,'PROCEDURE',\n",
    "                'HOSPITAL COURSE BY PROBLEM','COMPLICATIONS',\n",
    "                'CONSULTANTS', 'PHYSICAL EXAMINATION ON DISCHARGE',\n",
    "                'DISCHARGE MEDICATIONS', 'DISPOSITION', 'FOLLOW-UP APPOINTMENTS' ,'CODE STATUS', 'PRIMARY CARE PHYSICIAN']\n",
    "\n",
    "\n",
    "input_dir = './input/'\n",
    "output_dir = './output/'\n",
    "\n",
    "with open('final.csv','a',newline='') as f:\n",
    "    writer=csv.writer(f)\n",
    "    writer.writerow(columns)\n",
    "    #for eachfile in dir:\n",
    "    input_files = os.listdir(input_dir)\n",
    "    output_files = os.listdir(output_dir)\n",
    "    for i in range(len(input_files)):\n",
    "        input_file = input_dir+input_files[i]\n",
    "        output_file = output_dir+output_files[i]\n",
    "        entries_for_each_record(input_file,output_file)\n",
    "        \n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(input_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
